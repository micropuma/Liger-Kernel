from liger_kernel.transformers import AutoLigerKernelForCausalLM
from transformers import AutoTokenizer
import torch

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")  # ğŸ¯ ç»Ÿä¸€å®šä¹‰è®¾å¤‡
model_name = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"

# âœ… åŠ è½½å¹¶è¿ç§»æ¨¡å‹
model = AutoLigerKernelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map={"": device}  # âš ï¸ å…³é”®ï¼šå¼ºåˆ¶æ¨¡å‹ä¸»è®¾å¤‡
).eval()

# âœ… åŠ è½½Tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name)

# âœ… è¿ç§»è¾“å…¥æ•°æ®ï¼ˆè¦†ç›–æ‰€æœ‰å¼ é‡ï¼‰
input_text = "ç”¨é€šä¿—çš„ä¸­æ–‡è§£é‡Šä¸€ä¸‹é‡å­çº ç¼ "
inputs = tokenizer(input_text, return_tensors="pt")
inputs = {k: v.to(device) for k, v in inputs.items()}  # ğŸš€ å­—å…¸å†…å…¨é‡è¿ç§»

# âœ… ç”Ÿæˆæ—¶ä¼ é€’è®¾å¤‡å‚æ•°
with torch.no_grad():
    outputs = model.generate(
        **inputs,
        max_new_tokens=100,
        do_sample=True,
        temperature=0.7,
        top_k=50,
        top_p=0.9,
        eos_token_id=tokenizer.eos_token_id,      
    )

print("\nğŸš€ æ¨¡å‹ç”Ÿæˆè¾“å‡ºï¼š\n")
print(tokenizer.decode(outputs[0], skip_special_tokens=True))

